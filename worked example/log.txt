INFO       in modeling.py         --> Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
INFO       in modeling_bert.py    --> Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
INFO       in modeling_xlnet.py   --> Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
INFO       in registrable.py      --> instantiating registered subclass relu of <class 'allennlp.nn.activations.Activation'>
INFO       in registrable.py      --> instantiating registered subclass relu of <class 'allennlp.nn.activations.Activation'>
INFO       in registrable.py      --> instantiating registered subclass relu of <class 'allennlp.nn.activations.Activation'>
INFO       in registrable.py      --> instantiating registered subclass relu of <class 'allennlp.nn.activations.Activation'>
INFO       in tokenization.py     --> loading vocabulary file C:\Users\d23895jm\AppData\Local\ChemDataExtractor\ChemDataExtractor\models/scibert_cased_vocab-1.0.txt
INFO       in xptlpaper.py        --> Gathering initial files:
----------------------
INFO       in xptlpaper.py        --> Trying to read in manuscript as an html file
INFO       in xptlpaper.py        --> Manuscript not found as a html file, trying again as an xml.
INFO       in xptlpaper.py        --> Manuscript loaded in from S1385894723007039.xml
INFO       in xptlpaper.py        --> Files processed, ready to extract information
-------------------------------------
INFO       in table.py            --> Initialization of table: "[['pseudo-first-order', 'pseudo-first-order', 'pseudo-first-order', 'pseudo-first-order', 'pseudo-second-order', 'pseudo-second-order', 'pseudo-second-order', 'pseudo-second-order', 'pseudo-second-order'], ['K1 (min−1)', 'Qe.cal (mg·g−1)', 'R2', 'R2', 'K2', 'Qe.cal (mg·g−1)', 'Qe.cal (mg·g−1)', 'R2', 'R2'], ['0.181', '156.329', '0.904', '0.904', '0.002', '160.859', '160.859', '0.996', '0.996'], ['', '', '', '', '', '', '', '', ''], ['Intra-particle diffusion', 'Intra-particle diffusion', 'Intra-particle diffusion', 'Intra-particle diffusion', 'Intra-particle diffusion', 'Intra-particle diffusion', 'Intra-particle diffusion', 'Intra-particle diffusion', 'Intra-particle diffusion'], ['kdiff1 (mg·g−1 min0.5)', 'C1', 'R2', 'kdiff2 (mg·g−1 min0.5)', 'C2', 'R2', 'kdiff3 (mg·g−1 min0.5)', 'C3', 'R2'], ['17.941', '63.303', '0.937', '0.914', '144.790', '0.929', '0.226', '154.906', '0.890']]"
INFO       in table.py            --> Configuration parameters are: {'use_title_row': True, 'use_prefixing': True, 'use_footnotes': True, 'use_spanning_cells': True, 'use_header_extension': True, 'use_max_data_area': False, 'standardize_empty_data': True, 'row_header': None, 'col_header': None}
INFO       in from_any.py         --> Input is list type.
INFO       in from_any.py         --> Input is list type.
INFO       in from_any.py         --> Input is list type.
INFO       in table.py            --> Initialization of table: "[['Models', 'Parameters', 'Linear', 'Nonlinear'], ['Langmuir', 'Qmax (mg·g−1)', '414.6', '403.5'], ['Langmuir', 'KL (L·mg−1)', '0.0300', '0.0314'], ['Langmuir', 'R2', '0.998', '0.997'], ['Freundlich', 'KF', '41.17', '54.82'], ['Freundlich', 'n', '2.44', '0.437'], ['Freundlich', 'R2', '0.954', '0.958'], ['Temkin', 'b (J‧mol−1)', '30.924', '30.923'], ['Temkin', 'KT (L·mg−1)', '0.380', '0.377'], ['Temkin', 'R2', '0.996', '0.996']]"
INFO       in table.py            --> Configuration parameters are: {'use_title_row': True, 'use_prefixing': True, 'use_footnotes': True, 'use_spanning_cells': True, 'use_header_extension': True, 'use_max_data_area': False, 'standardize_empty_data': True, 'row_header': None, 'col_header': None}
INFO       in from_any.py         --> Input is list type.
INFO       in from_any.py         --> Input is list type.
INFO       in from_any.py         --> Input is list type.
INFO       in table.py            --> Initialization of table: "[['Adsorbent', 'Adsorption capacity (mg·g−1)', 'Reference'], ['PAN/Ppy core shell NFs', '75', ''], ['PAN-NH2 NFs', '137', ''], ['CCN-PEI NFs', '358.4', ''], ['CS-PGMA-PEI NFs', '139', ''], ['PAN/CA/MIL-125/TiO2 NFs', '273.3', ''], ['ZIF-8@ZIF-8/PAN NFs', '39.7', ''], ['PVA/PEI NFs', '150', ''], ['CA/PEI NFs', '285.7', ''], ['PAN/HPEI NFs', '206', ''], ['Co-Al-LDH@Fe2O3/3DPC NFs', '400.4', ''], ['ZIF-8@PDA/PAN NFs', '212.7', ''], ['PPy@PI NFs', '162.8', ''], ['PAN/PEI@ZIF-8 NFs', '403.5', 'This work']]"
INFO       in table.py            --> Configuration parameters are: {'use_title_row': True, 'use_prefixing': True, 'use_footnotes': True, 'use_spanning_cells': True, 'use_header_extension': True, 'use_max_data_area': False, 'standardize_empty_data': True, 'row_header': None, 'col_header': None}
INFO       in from_any.py         --> Input is list type.
INFO       in from_any.py         --> Input is list type.
INFO       in from_any.py         --> Input is list type.
INFO       in table.py            --> Initialization of table: "[['Tem. (K)', 'Ln Kd', 'ΔG(kJ‧ mol−1)', 'ΔH(kJ‧mol−1)', 'ΔS(JK−1‧mol−1)'], ['298', '1.1538', '−0.355', '8.542', '29.729'], ['308', '1.2324', '−0.535', '8.542', '29.729'], ['318', '1.4347', '−0.954', '8.542', '29.729']]"
INFO       in table.py            --> Configuration parameters are: {'use_title_row': True, 'use_prefixing': True, 'use_footnotes': True, 'use_spanning_cells': True, 'use_header_extension': True, 'use_max_data_area': False, 'standardize_empty_data': True, 'row_header': None, 'col_header': None}
INFO       in from_any.py         --> Input is list type.
INFO       in from_any.py         --> Input is list type.
INFO       in from_any.py         --> Input is list type.
INFO       in table.py            --> Initialization of table: "[['pseudo-first-order'], ['K1 (min−1)'], ['0.181'], ['Intra-particle diffusion'], ['kdiff1 (mg·g−1 min0.5)'], ['17.941'], ['17.941'], ['17.941'], ['17.941'], ['17.941'], ['17.941'], ['17.941'], ['17.941']]"
INFO       in table.py            --> Configuration parameters are: {'standardize_empty_data': False, 'clean_row_header': True, 'row_header': 0, 'col_header': 4}
INFO       in from_any.py         --> Input is list type.
INFO       in from_any.py         --> Input is list type.
INFO       in table.py            --> Initialization of table: "[['pseudo-first-order'], ['K1 (min−1)'], ['0.181'], ['Intra-particle diffusion'], ['kdiff1 (mg·g−1 min0.5)'], ['17.941'], ['17.941'], ['17.941'], ['17.941'], ['17.941'], ['17.941'], ['17.941'], ['17.941']]"
INFO       in table.py            --> Configuration parameters are: {'standardize_empty_data': False, 'clean_row_header': True, 'row_header': 0, 'col_header': 4}
INFO       in from_any.py         --> Input is list type.
INFO       in from_any.py         --> Input is list type.
INFO       in table.py            --> Initialization of table: "[['pseudo-first-order'], ['K1 (min−1)'], ['0.181'], ['Intra-particle diffusion'], ['kdiff1 (mg·g−1 min0.5)'], ['17.941'], ['17.941'], ['17.941'], ['17.941'], ['17.941'], ['17.941'], ['17.941'], ['17.941']]"
INFO       in table.py            --> Configuration parameters are: {'standardize_empty_data': False, 'clean_row_header': True, 'row_header': 0, 'col_header': 4}
INFO       in from_any.py         --> Input is list type.
INFO       in from_any.py         --> Input is list type.
INFO       in table.py            --> Initialization of table: "[['pseudo-first-order'], ['K1 (min−1)'], ['0.181'], ['Intra-particle diffusion'], ['kdiff1 (mg·g−1 min0.5)'], ['']]"
INFO       in table.py            --> Configuration parameters are: {'standardize_empty_data': False, 'clean_row_header': True, 'row_header': 0, 'col_header': 4}
INFO       in from_any.py         --> Input is list type.
INFO       in from_any.py         --> Input is list type.
INFO       in table.py            --> Initialization of table: "[['Models', 'Parameters'], ['Langmuir', 'Qmax (mg·g−1)'], ['Langmuir', 'Qmax (mg·g−1)'], ['Langmuir', 'KL (L·mg−1)'], ['Langmuir', 'KL (L·mg−1)'], ['Langmuir', 'R2'], ['Langmuir', 'R2'], ['Freundlich', 'KF'], ['Freundlich', 'KF'], ['Freundlich', 'n'], ['Freundlich', 'n'], ['Freundlich', 'R2'], ['Freundlich', 'R2'], ['Temkin', 'b (J‧mol−1)'], ['Temkin', 'b (J‧mol−1)'], ['Temkin', 'KT (L·mg−1)'], ['Temkin', 'KT (L·mg−1)'], ['Temkin', 'R2'], ['Temkin', 'R2']]"
INFO       in table.py            --> Configuration parameters are: {'standardize_empty_data': False, 'clean_row_header': True, 'row_header': 0, 'col_header': 0}
INFO       in from_any.py         --> Input is list type.
INFO       in from_any.py         --> Input is list type.
INFO       in table.py            --> Initialization of table: "[['Models', 'Parameters'], ['Langmuir', 'Qmax (mg·g−1)'], ['Langmuir', 'Qmax (mg·g−1)'], ['Langmuir', 'KL (L·mg−1)'], ['Langmuir', 'KL (L·mg−1)'], ['Langmuir', 'R2'], ['Langmuir', 'R2'], ['Freundlich', 'KF'], ['Freundlich', 'KF'], ['Freundlich', 'n'], ['Freundlich', 'n'], ['Freundlich', 'R2'], ['Freundlich', 'R2'], ['Temkin', 'b (J‧mol−1)'], ['Temkin', 'b (J‧mol−1)'], ['Temkin', 'KT (L·mg−1)'], ['Temkin', 'KT (L·mg−1)'], ['Temkin', 'R2'], ['Temkin', 'R2']]"
INFO       in table.py            --> Configuration parameters are: {'standardize_empty_data': False, 'clean_row_header': True, 'row_header': 0, 'col_header': 0}
INFO       in from_any.py         --> Input is list type.
INFO       in from_any.py         --> Input is list type.
INFO       in table.py            --> Initialization of table: "[['Models', 'Parameters'], ['Langmuir', 'Qmax (mg·g−1)'], ['Langmuir', 'Qmax (mg·g−1)'], ['Langmuir', 'KL (L·mg−1)'], ['Langmuir', 'KL (L·mg−1)'], ['Langmuir', 'R2'], ['Langmuir', 'R2'], ['Freundlich', 'KF'], ['Freundlich', 'KF'], ['Freundlich', 'n'], ['Freundlich', 'n'], ['Freundlich', 'R2'], ['Freundlich', 'R2'], ['Temkin', 'b (J‧mol−1)'], ['Temkin', 'b (J‧mol−1)'], ['Temkin', 'KT (L·mg−1)'], ['Temkin', 'KT (L·mg−1)'], ['Temkin', 'R2'], ['Temkin', 'R2']]"
INFO       in table.py            --> Configuration parameters are: {'standardize_empty_data': False, 'clean_row_header': True, 'row_header': 0, 'col_header': 0}
INFO       in from_any.py         --> Input is list type.
INFO       in from_any.py         --> Input is list type.
INFO       in table.py            --> Initialization of table: "[['Models'], ['Langmuir'], ['Langmuir'], ['Langmuir'], ['Freundlich'], ['Freundlich'], ['Freundlich'], ['Temkin'], ['Temkin'], ['Temkin']]"
INFO       in table.py            --> Configuration parameters are: {'standardize_empty_data': False, 'clean_row_header': True, 'row_header': 0, 'col_header': 0}
INFO       in from_any.py         --> Input is list type.
INFO       in from_any.py         --> Input is list type.
INFO       in table.py            --> Initialization of table: "[['Models'], ['Langmuir'], ['Langmuir'], ['Langmuir'], ['Freundlich'], ['Freundlich'], ['Freundlich'], ['Temkin'], ['Temkin'], ['Temkin']]"
INFO       in table.py            --> Configuration parameters are: {'standardize_empty_data': False, 'clean_row_header': True, 'row_header': 0, 'col_header': 0}
INFO       in from_any.py         --> Input is list type.
INFO       in from_any.py         --> Input is list type.
INFO       in table.py            --> Initialization of table: "[['Models'], ['Langmuir'], ['Langmuir'], ['Langmuir'], ['Freundlich'], ['Freundlich'], ['Freundlich'], ['Temkin'], ['Temkin'], ['Temkin']]"
INFO       in table.py            --> Configuration parameters are: {'standardize_empty_data': False, 'clean_row_header': True, 'row_header': 0, 'col_header': 0}
INFO       in from_any.py         --> Input is list type.
INFO       in from_any.py         --> Input is list type.
INFO       in table.py            --> Initialization of table: "[['Models'], [''], [''], ['']]"
INFO       in table.py            --> Configuration parameters are: {'standardize_empty_data': False, 'clean_row_header': True, 'row_header': 0, 'col_header': 0}
INFO       in from_any.py         --> Input is list type.
INFO       in from_any.py         --> Input is list type.
INFO       in table.py            --> Initialization of table: "[['Adsorbent'], ['PAN/Ppy core shell NFs'], ['PAN/Ppy core shell NFs'], ['PAN-NH2 NFs'], ['PAN-NH2 NFs'], ['CCN-PEI NFs'], ['CCN-PEI NFs'], ['CS-PGMA-PEI NFs'], ['CS-PGMA-PEI NFs'], ['PAN/CA/MIL-125/TiO2 NFs'], ['PAN/CA/MIL-125/TiO2 NFs'], ['ZIF-8@ZIF-8/PAN NFs'], ['ZIF-8@ZIF-8/PAN NFs'], ['PVA/PEI NFs'], ['PVA/PEI NFs'], ['CA/PEI NFs'], ['CA/PEI NFs'], ['PAN/HPEI NFs'], ['PAN/HPEI NFs'], ['Co-Al-LDH@Fe2O3/3DPC NFs'], ['Co-Al-LDH@Fe2O3/3DPC NFs'], ['ZIF-8@PDA/PAN NFs'], ['ZIF-8@PDA/PAN NFs'], ['PPy@PI NFs'], ['PPy@PI NFs'], ['PAN/PEI@ZIF-8 NFs'], ['PAN/PEI@ZIF-8 NFs']]"
INFO       in table.py            --> Configuration parameters are: {'standardize_empty_data': False, 'clean_row_header': True, 'row_header': 0, 'col_header': 0}
INFO       in from_any.py         --> Input is list type.
INFO       in from_any.py         --> Input is list type.
INFO       in table.py            --> Initialization of table: "[['Adsorbent'], ['PAN/Ppy core shell NFs'], ['PAN/Ppy core shell NFs'], ['PAN-NH2 NFs'], ['PAN-NH2 NFs'], ['CCN-PEI NFs'], ['CCN-PEI NFs'], ['CS-PGMA-PEI NFs'], ['CS-PGMA-PEI NFs'], ['PAN/CA/MIL-125/TiO2 NFs'], ['PAN/CA/MIL-125/TiO2 NFs'], ['ZIF-8@ZIF-8/PAN NFs'], ['ZIF-8@ZIF-8/PAN NFs'], ['PVA/PEI NFs'], ['PVA/PEI NFs'], ['CA/PEI NFs'], ['CA/PEI NFs'], ['PAN/HPEI NFs'], ['PAN/HPEI NFs'], ['Co-Al-LDH@Fe2O3/3DPC NFs'], ['Co-Al-LDH@Fe2O3/3DPC NFs'], ['ZIF-8@PDA/PAN NFs'], ['ZIF-8@PDA/PAN NFs'], ['PPy@PI NFs'], ['PPy@PI NFs'], ['PAN/PEI@ZIF-8 NFs'], ['PAN/PEI@ZIF-8 NFs']]"
INFO       in table.py            --> Configuration parameters are: {'standardize_empty_data': False, 'clean_row_header': True, 'row_header': 0, 'col_header': 0}
INFO       in from_any.py         --> Input is list type.
INFO       in from_any.py         --> Input is list type.
INFO       in table.py            --> Initialization of table: "[['Adsorbent'], ['PAN/Ppy core shell NFs'], ['PAN/Ppy core shell NFs'], ['PAN-NH2 NFs'], ['PAN-NH2 NFs'], ['CCN-PEI NFs'], ['CCN-PEI NFs'], ['CS-PGMA-PEI NFs'], ['CS-PGMA-PEI NFs'], ['PAN/CA/MIL-125/TiO2 NFs'], ['PAN/CA/MIL-125/TiO2 NFs'], ['ZIF-8@ZIF-8/PAN NFs'], ['ZIF-8@ZIF-8/PAN NFs'], ['PVA/PEI NFs'], ['PVA/PEI NFs'], ['CA/PEI NFs'], ['CA/PEI NFs'], ['PAN/HPEI NFs'], ['PAN/HPEI NFs'], ['Co-Al-LDH@Fe2O3/3DPC NFs'], ['Co-Al-LDH@Fe2O3/3DPC NFs'], ['ZIF-8@PDA/PAN NFs'], ['ZIF-8@PDA/PAN NFs'], ['PPy@PI NFs'], ['PPy@PI NFs'], ['PAN/PEI@ZIF-8 NFs'], ['PAN/PEI@ZIF-8 NFs']]"
INFO       in table.py            --> Configuration parameters are: {'standardize_empty_data': False, 'clean_row_header': True, 'row_header': 0, 'col_header': 0}
INFO       in from_any.py         --> Input is list type.
INFO       in from_any.py         --> Input is list type.
INFO       in table.py            --> Initialization of table: "[['Adsorbent'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['']]"
INFO       in table.py            --> Configuration parameters are: {'standardize_empty_data': False, 'clean_row_header': True, 'row_header': 0, 'col_header': 0}
INFO       in from_any.py         --> Input is list type.
INFO       in from_any.py         --> Input is list type.
INFO       in table.py            --> Initialization of table: "[['Tem. (K)'], ['298'], ['298'], ['298'], ['298'], ['308'], ['308'], ['308'], ['308'], ['318'], ['318'], ['318'], ['318']]"
INFO       in table.py            --> Configuration parameters are: {'standardize_empty_data': False, 'clean_row_header': True, 'row_header': 0, 'col_header': 0}
INFO       in from_any.py         --> Input is list type.
INFO       in from_any.py         --> Input is list type.
INFO       in table.py            --> Initialization of table: "[['Tem. (K)'], ['298'], ['298'], ['298'], ['298'], ['308'], ['308'], ['308'], ['308'], ['318'], ['318'], ['318'], ['318']]"
INFO       in table.py            --> Configuration parameters are: {'standardize_empty_data': False, 'clean_row_header': True, 'row_header': 0, 'col_header': 0}
INFO       in from_any.py         --> Input is list type.
INFO       in from_any.py         --> Input is list type.
INFO       in table.py            --> Initialization of table: "[['Tem. (K)'], ['298'], ['298'], ['298'], ['298'], ['308'], ['308'], ['308'], ['308'], ['318'], ['318'], ['318'], ['318']]"
INFO       in table.py            --> Configuration parameters are: {'standardize_empty_data': False, 'clean_row_header': True, 'row_header': 0, 'col_header': 0}
INFO       in from_any.py         --> Input is list type.
INFO       in from_any.py         --> Input is list type.
INFO       in table.py            --> Initialization of table: "[['Tem. (K)'], [''], [''], ['']]"
INFO       in table.py            --> Configuration parameters are: {'standardize_empty_data': False, 'clean_row_header': True, 'row_header': 0, 'col_header': 0}
INFO       in from_any.py         --> Input is list type.
INFO       in from_any.py         --> Input is list type.
INFO       in archival.py         --> loading archive file C:\Users\d23895jm\AppData\Local\ChemDataExtractor\ChemDataExtractor\models/bert_finetuned_crf_model-1.0a
WARNING    in params.py           --> _jsonnet not loaded, treating C:\Users\d23895jm\AppData\Local\ChemDataExtractor\ChemDataExtractor\models/bert_finetuned_crf_model-1.0a\config.json as json
WARNING    in params.py           --> _jsonnet not loaded, treating snippet as json
INFO       in registrable.py      --> instantiating registered subclass bert_crf_tagger of <class 'allennlp.models.model.Model'>
INFO       in params.py           --> type = default
INFO       in registrable.py      --> instantiating registered subclass default of <class 'allennlp.data.vocabulary.Vocabulary'>
INFO       in vocabulary.py       --> Loading token dictionary from C:\Users\d23895jm\AppData\Local\ChemDataExtractor\ChemDataExtractor\models/bert_finetuned_crf_model-1.0a\vocabulary.
INFO       in from_params.py      --> instantiating class <class 'allennlp.models.model.Model'> from params {'type': 'bert_crf_tagger', 'calculate_span_f1': True, 'constrain_crf_decoding': True, 'dropout': 0.1, 'include_start_end_transitions': False, 'label_encoding': 'BIO', 'text_field_embedder': {'embedder_to_indexer_map': {'bert': ['bert', 'bert-offsets']}, 'allow_unmatched_keys': True, 'token_embedders': {'bert': {'type': 'bert-pretrained', 'top_layer_only': True, 'requires_grad': True, 'pretrained_model': 'C:\\Users\\d23895jm\\AppData\\Local\\ChemDataExtractor\\ChemDataExtractor\\models/scibert_cased_weights-1.0.tar.gz'}}}} and extras {'vocab'}
INFO       in params.py           --> model.type = bert_crf_tagger
INFO       in from_params.py      --> instantiating class <class 'chemdataextractor.nlp.finetuned_bert_crf_wrapper._BertCrfTagger'> from params {'calculate_span_f1': True, 'constrain_crf_decoding': True, 'dropout': 0.1, 'include_start_end_transitions': False, 'label_encoding': 'BIO', 'text_field_embedder': {'embedder_to_indexer_map': {'bert': ['bert', 'bert-offsets']}, 'allow_unmatched_keys': True, 'token_embedders': {'bert': {'type': 'bert-pretrained', 'top_layer_only': True, 'requires_grad': True, 'pretrained_model': 'C:\\Users\\d23895jm\\AppData\\Local\\ChemDataExtractor\\ChemDataExtractor\\models/scibert_cased_weights-1.0.tar.gz'}}}} and extras {'vocab'}
INFO       in from_params.py      --> instantiating class <class 'allennlp.modules.text_field_embedders.text_field_embedder.TextFieldEmbedder'> from params {'embedder_to_indexer_map': {'bert': ['bert', 'bert-offsets']}, 'allow_unmatched_keys': True, 'token_embedders': {'bert': {'type': 'bert-pretrained', 'top_layer_only': True, 'requires_grad': True, 'pretrained_model': 'C:\\Users\\d23895jm\\AppData\\Local\\ChemDataExtractor\\ChemDataExtractor\\models/scibert_cased_weights-1.0.tar.gz'}}} and extras {'vocab'}
INFO       in params.py           --> model.text_field_embedder.type = basic
INFO       in params.py           --> model.text_field_embedder.allow_unmatched_keys = True
INFO       in from_params.py      --> instantiating class <class 'allennlp.modules.token_embedders.token_embedder.TokenEmbedder'> from params {'type': 'bert-pretrained', 'top_layer_only': True, 'requires_grad': True, 'pretrained_model': 'C:\\Users\\d23895jm\\AppData\\Local\\ChemDataExtractor\\ChemDataExtractor\\models/scibert_cased_weights-1.0.tar.gz'} and extras {'vocab'}
INFO       in params.py           --> model.text_field_embedder.token_embedders.bert.type = bert-pretrained
INFO       in from_params.py      --> instantiating class <class 'allennlp.modules.token_embedders.bert_token_embedder.PretrainedBertEmbedder'> from params {'top_layer_only': True, 'requires_grad': True, 'pretrained_model': 'C:\\Users\\d23895jm\\AppData\\Local\\ChemDataExtractor\\ChemDataExtractor\\models/scibert_cased_weights-1.0.tar.gz'} and extras {'vocab'}
INFO       in params.py           --> model.text_field_embedder.token_embedders.bert.pretrained_model = C:\Users\d23895jm\AppData\Local\ChemDataExtractor\ChemDataExtractor\models/scibert_cased_weights-1.0.tar.gz
INFO       in params.py           --> model.text_field_embedder.token_embedders.bert.requires_grad = True
INFO       in params.py           --> model.text_field_embedder.token_embedders.bert.top_layer_only = True
INFO       in params.py           --> model.text_field_embedder.token_embedders.bert.scalar_mix_parameters = None
INFO       in modeling.py         --> loading archive file C:\Users\d23895jm\AppData\Local\ChemDataExtractor\ChemDataExtractor\models/scibert_cased_weights-1.0.tar.gz
INFO       in modeling.py         --> extracting archive file C:\Users\d23895jm\AppData\Local\ChemDataExtractor\ChemDataExtractor\models/scibert_cased_weights-1.0.tar.gz to temp dir C:\Users\d23895jm\AppData\Local\Temp\tmp0czuh1ey
INFO       in modeling.py         --> Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 31116
}

INFO       in params.py           --> model.label_namespace = labels
INFO       in params.py           --> model.label_encoding = BIO
INFO       in params.py           --> model.include_start_end_transitions = False
INFO       in params.py           --> model.constrain_crf_decoding = True
INFO       in params.py           --> model.calculate_span_f1 = True
INFO       in params.py           --> model.dropout = 0.1
INFO       in params.py           --> model.verbose_metrics = False
INFO       in initializers.py     --> Initializing parameters
INFO       in initializers.py     --> Done initializing parameters; the following parameters are using their default initialization from their code
INFO       in initializers.py     -->    crf._constraint_mask
INFO       in initializers.py     -->    crf.transitions
INFO       in initializers.py     -->    tag_projection_layer._module.bias
INFO       in initializers.py     -->    tag_projection_layer._module.weight
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.embeddings.LayerNorm.bias
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.embeddings.LayerNorm.weight
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.embeddings.position_embeddings.weight
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.embeddings.token_type_embeddings.weight
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.embeddings.word_embeddings.weight
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.attention.output.LayerNorm.bias
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.attention.output.LayerNorm.weight
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.attention.output.dense.bias
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.attention.output.dense.weight
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.attention.self.key.bias
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.attention.self.key.weight
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.attention.self.query.bias
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.attention.self.query.weight
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.attention.self.value.bias
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.attention.self.value.weight
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.intermediate.dense.bias
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.intermediate.dense.weight
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.output.LayerNorm.bias
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.output.LayerNorm.weight
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.output.dense.bias
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.output.dense.weight
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.attention.output.LayerNorm.bias
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.attention.output.LayerNorm.weight
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.attention.output.dense.bias
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.attention.output.dense.weight
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.attention.self.key.bias
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.attention.self.key.weight
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.attention.self.query.bias
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.attention.self.query.weight
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.attention.self.value.bias
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.attention.self.value.weight
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.intermediate.dense.bias
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.intermediate.dense.weight
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.output.LayerNorm.bias
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.output.LayerNorm.weight
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.output.dense.bias
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.output.dense.weight
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.output.LayerNorm.bias
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.output.LayerNorm.weight
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.output.dense.bias
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.output.dense.weight
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.self.key.bias
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.self.key.weight
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.self.query.bias
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.self.query.weight
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.self.value.bias
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.self.value.weight
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.intermediate.dense.bias
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.intermediate.dense.weight
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.output.LayerNorm.bias
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.output.LayerNorm.weight
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.output.dense.bias
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.output.dense.weight
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.output.LayerNorm.bias
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.output.LayerNorm.weight
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.output.dense.bias
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.output.dense.weight
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.self.key.bias
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.self.key.weight
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.self.query.bias
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.self.query.weight
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.self.value.bias
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.self.value.weight
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.intermediate.dense.bias
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.intermediate.dense.weight
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.output.LayerNorm.bias
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.output.LayerNorm.weight
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.output.dense.bias
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.output.dense.weight
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.attention.output.LayerNorm.bias
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.attention.output.LayerNorm.weight
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.attention.output.dense.bias
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.attention.output.dense.weight
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.attention.self.key.bias
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.attention.self.key.weight
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.attention.self.query.bias
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.attention.self.query.weight
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.attention.self.value.bias
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.attention.self.value.weight
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.intermediate.dense.bias
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.intermediate.dense.weight
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.output.LayerNorm.bias
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.output.LayerNorm.weight
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.output.dense.bias
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.output.dense.weight
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.attention.output.LayerNorm.bias
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.attention.output.LayerNorm.weight
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.attention.output.dense.bias
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.attention.output.dense.weight
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.attention.self.key.bias
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.attention.self.key.weight
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.attention.self.query.bias
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.attention.self.query.weight
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.attention.self.value.bias
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.attention.self.value.weight
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.intermediate.dense.bias
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.intermediate.dense.weight
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.output.LayerNorm.bias
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.output.LayerNorm.weight
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.output.dense.bias
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.output.dense.weight
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.attention.output.LayerNorm.bias
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.attention.output.LayerNorm.weight
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.attention.output.dense.bias
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.attention.output.dense.weight
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.attention.self.key.bias
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.attention.self.key.weight
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.attention.self.query.bias
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.attention.self.query.weight
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.attention.self.value.bias
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.attention.self.value.weight
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.intermediate.dense.bias
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.intermediate.dense.weight
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.output.LayerNorm.bias
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.output.LayerNorm.weight
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.output.dense.bias
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.output.dense.weight
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.attention.output.LayerNorm.bias
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.attention.output.LayerNorm.weight
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.attention.output.dense.bias
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.attention.output.dense.weight
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.attention.self.key.bias
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.attention.self.key.weight
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.attention.self.query.bias
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.attention.self.query.weight
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.attention.self.value.bias
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.attention.self.value.weight
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.intermediate.dense.bias
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.intermediate.dense.weight
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.output.LayerNorm.bias
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.output.LayerNorm.weight
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.output.dense.bias
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.output.dense.weight
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.attention.output.LayerNorm.bias
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.attention.output.LayerNorm.weight
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.attention.output.dense.bias
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.attention.output.dense.weight
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.attention.self.key.bias
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.attention.self.key.weight
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.attention.self.query.bias
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.attention.self.query.weight
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.attention.self.value.bias
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.attention.self.value.weight
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.intermediate.dense.bias
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.intermediate.dense.weight
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.output.LayerNorm.bias
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.output.LayerNorm.weight
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.output.dense.bias
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.output.dense.weight
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.attention.output.LayerNorm.bias
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.attention.output.LayerNorm.weight
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.attention.output.dense.bias
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.attention.output.dense.weight
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.attention.self.key.bias
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.attention.self.key.weight
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.attention.self.query.bias
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.attention.self.query.weight
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.attention.self.value.bias
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.attention.self.value.weight
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.intermediate.dense.bias
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.intermediate.dense.weight
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.output.LayerNorm.bias
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.output.LayerNorm.weight
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.output.dense.bias
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.output.dense.weight
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.attention.output.LayerNorm.bias
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.attention.output.LayerNorm.weight
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.attention.output.dense.bias
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.attention.output.dense.weight
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.attention.self.key.bias
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.attention.self.key.weight
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.attention.self.query.bias
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.attention.self.query.weight
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.attention.self.value.bias
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.attention.self.value.weight
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.intermediate.dense.bias
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.intermediate.dense.weight
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.output.LayerNorm.bias
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.output.LayerNorm.weight
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.output.dense.bias
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.output.dense.weight
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.attention.output.LayerNorm.bias
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.attention.output.LayerNorm.weight
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.attention.output.dense.bias
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.attention.output.dense.weight
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.attention.self.key.bias
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.attention.self.key.weight
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.attention.self.query.bias
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.attention.self.query.weight
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.attention.self.value.bias
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.attention.self.value.weight
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.intermediate.dense.bias
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.intermediate.dense.weight
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.output.LayerNorm.bias
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.output.LayerNorm.weight
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.output.dense.bias
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.output.dense.weight
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.pooler.dense.bias
INFO       in initializers.py     -->    text_field_embedder.token_embedder_bert.bert_model.pooler.dense.weight
